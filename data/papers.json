{
  "papers": [
    {
      "title": "RealisID: Scale-Robust and Fine-Controllable Identity Customization via Local and Global Complementation",
      "authors": "Zhaoyang Sun, Fei Du, Weihua Chen, Fan Wang, Yaxiong Chen, Yi Rong, Shengwu Xiong",
      "conference": "The 39th Annual AAAI Conference on Artificial Intelligence",
      "pdfLink": "https://arxiv.org/pdf/2412.16832",
      "codeLink": "https://arxiv.org/pdf/2412.16832",
      "abs": "Recently, the success of text-to-image synthesis has greatly advanced the development of identity customization techniques, whose main goal is to produce realistic identityspecific photographs based on text prompts and reference face images. However, it is difficult for existing identity customization methods to simultaneously meet the various requirements of different real-world applications, including the identity fidelity of small face, the control of face location, pose and expression, as well as the customization of multiple persons. To this end, we propose a scale-robust and finecontrollable method, namely RealisID, which learns different control capabilities through the cooperation between a pair of local and global branches. Specifically, by using cropping and up-sampling operations to filter out face-irrelevant information, the local branch concentrates the fine control of facial details and the scale-robust identity fidelity within the face region. Meanwhile, the global branch manages the overall harmony of the entire image. It also controls the face location by taking the location guidance as input. As a result, RealisID can benefit from the complementarity of these two branches. Finally, by implementing our branches with two different variants of ControlNet, our method can be easily extended to handle multi-person customization, even only trained on single-person datasets. Extensive experiments and ablation studies indicate the effectiveness of RealisID and verify its ability in fulfilling all the requirements mentioned above.",
      "bibtex": "@article{sun2024realisid,title={RealisID: Scale-Robust and Fine-Controllable Identity Customization via Local and Global Complementation},author={Sun, Zhaoyang and Du, Fei and Chen, Weihua and Wang, Fan and Chen, Yaxiong and Rong, Yi and Xiong, Shengwu},journal={arXiv preprint arXiv:2412.16832},year={2024}}"
    },
    {
      "title": "Hyperspectral Image Classification via Cascaded Spatial Cross-Attention Network",
      "authors": "Bo Zhang, Yaxiong Chen, Shengwu Xiong, Xiaoqiang Lu",
      "conference": "lEEE Transactions on Image Processing",
      "pdfLink": "https://ieeexplore.ieee.org/document/10857952/authors#authors",
      "codeLink": "https://github.com/WUTCM-Lab/CSCANet",
      "abs": "In hyperspectral images (HSIs), different land cover (LC) classes have distinct reflective characteristics at various wavelengths. Therefore, relying on only a few bands to distinguish all LC classes often leads to information loss, resulting in poor average accuracy. To address this problem, we propose a method called Cascaded Spatial Cross-Attention Network (CSCANet) for HSI classification. We design a cascaded spatial cross-attention module, which first performs cross-attention on local and global features in the spatial context, then uses a group cascade structure to sequentially propagate important spatial regions within the different channels, and finally obtains joint attention features to improve the robustness of the network. Moreover, we also design a two-branch feature separation structure based on spatial-spectral features to separate different LC Tokens as much as possible, thereby improving the distinguishability of different LC classes. Extensive experiments demonstrate that our method achieves excellent performance in enhancing classification accuracy and robustness. The source code can be obtained from https://github.com/WUTCM-Lab/CSCANet.",
      "bibtex": "@ARTICLE{10857952,author={Zhang, Bo and Chen, Yaxiong and Xiong, Shengwu and Lu, Xiaoqiang},journal={IEEE Transactions on Image Processing}, title={Hyperspectral Image Classification via Cascaded Spatial Cross-Attention Network}, year={2025},volume={34},number={},pages={899-913},keywords={Feature extraction;Transformers;Data mining;Reflectivity;Hyperspectral imaging;Image classification;Accuracy;Artificial intelligence;Technological innovation;Sun;Hyperspectral image classification;group cascade structure;spatial cross-attention;spatial-spectral feature extraction},doi={10.1109/TIP.2025.3533205}}"
    },
    {
      "title": "Progressive Language-Aware Encoding and Decoding for Referring Expression Comprehension",
      "authors": "Yichen Zhao, Yaxiong Chen, Yi Rong, Shengwu Xiong",
      "conference": "Science China-Information Sciences",
      "pdfLink": "https://www.sciengine.com/doi/pdfView/11A9516284184402ABBD2610EDDA606E",
      "codeLink": "https://www.sciengine.com/doi/pdfView/11A9516284184402ABBD2610EDDA606E",
      "abs": "Referring expression comprehension (REC) seeks to locate the visual objects referred to by linguistic expressions, relying on multimodal fusion and reasoning for accurate interpretation and response to the indicated objects. However, the limitation arises from the prevailing practice of employing languageagnostic visual backbones for feature extraction in existing methods. This approach restricts the semantic representation of visual features, thereby resulting in suboptimal performance in multimodal fusion and reasoning. Moreover, fusion and reasoning modules commonly wear multiple hats, being trained from scratch on limited data, which increases the diﬃculty of model training and optimization. To this end, we propose a REC framework called PLAED, which is based on semantic-aware visual encoding to generate more reasonable visual features and progressive two-stage decoding to achieve eﬃcient target grounding. In addition, we implement informative sample learning to facilitate the model’s understanding of complex multimodal relationships. Speciﬁcally, the semantic-aware visual encoding is implemented by Dynamic Semantic Awareness Gate and Semantics-Boost Transformer. The two gradually improve the plausibility and accuracy of visual features by focusing on crucial visual elements mentioned by expressions. Based on enhanced visual features, a Progressive Two-Stage Decoder is introduced to establish deep associations between modalities, eﬀectively capturing information about target objects. Extensive experiments on four benchmarks conﬁrm that the proposed model is eﬀective and achieves state-of-the-art performance.",
      "bibtex": "@article{:/publisher/Science China Press/journal/SCIENCE CHINA Information Sciences///10.1007/s11432-024-4312-9,author = {Zhao Yichen,Chen Yaxiong,Rong Yi,Xiong Shengwu},title = {Progressive Language-Aware Encoding and Decoding for Referring Expression Comprehension},journal = {SCIENCE CHINA Information Sciences},year = {2025},pages = {-},url = {http://www.sciengine.com/publisher/Science China Press/journal/SCIENCE CHINA Information Sciences///10.1007/s11432-024-4312-9},doi = {https://doi.org/10.1007/s11432-024-4312-9}}"
    }
  ]
}
